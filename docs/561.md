# 现代硬件

> 原文:[https://jen kov . com/tutorials/Java-performance/modern-hardware . html](https://jenkov.com/tutorials/java-performance/modern-hardware.html)

为了让 Java 应用程序发挥最大性能，您需要了解它运行的硬件。当您编写符合硬件工作方式的代码时，您可以获得更高的性能。马丁·汤普森称之为“机械同情”。

当然你不需要了解你硬件的每一个小细节。因此，本教程侧重于你应该知道的部分。

## 现代硬件架构

现代计算机往往具有以下组件:

*   CPU -一个或多个。
*   一级缓存
*   L2 高速缓存
*   L3 缓存
*   主存储器
*   磁盘(高清)
*   网卡(NIC)

L1、L2 和 L3 是“一级”、“二级”和“三级”的简称。

上面提到的硬件组件如下所示:

![Modern hardware architecture showing CPUs, L1, L2 and L3 cache, main memory (RAM) and hard disks and network cards.](../Images/788a1ada959a3f5a0e617097c57db19b.png)

下面将更详细地解释每个部分。

### 中央处理器（central processing units 的缩写）

现代计算机可能有不止一个 CPU。事实上，这是计算机拥有多个 CPU 的普遍现象，而不是例外。

CPU 有一个指令缓存和数据缓存(L1)。指令缓存用于读取程序的指令。当 CPU 执行紧循环时，从指令缓存中读取循环内的指令比从主存中读取指令更快。因此，紧密的小循环比不适合指令高速缓存的大循环要快。

### L1 和 L2 高速缓存

每个 CPU 都有一个 L1 和 L2 内存缓存。L1 缓存通常很小，但 CPU 读取速度非常快。L2 缓存通常较大，但读取速度也比 L1 缓存慢。

当 CPU 从主存中读取数据时，它会将一块数据读入其 L2 和 L1 缓存中。从主内存中一次读取更大的内存块比一次读取一个字节(或 64 位)要快。一旦数据进入 L1 和 L2 缓存，CPU 访问它们的速度就会快得多。如果 CPU 更改了 CPU 缓存中的任何数据，这些数据需要在某个时候写回主内存。

### L3 缓存

L3(3 级)高速缓存通常是位于主内存顶部的高速缓存层。因此，三级高速缓存由主板上的所有 CPU 共享。每当 CPU 需要主内存中的数据时，就会将更大的数据块读入 L3 缓存。如果 CPU 需要在它先前请求的内存之后连续存储更多数据，很可能可以在 L3 缓存中找到内存。

### 硬盘

硬盘用于存储服务器重启后仍能保存的数据。硬盘比内存慢很多很多(虽然这即将改变)。但是硬盘的存储容量也比内存大得多。

硬盘分为两类:旋转硬盘和固态硬盘。旋转硬盘通常比固态硬盘更便宜、更大，但通常也更慢。

### 3D XPoint 内存

2015 年，英特尔宣布了一种称为“3D XPoint 内存”(发音为“交叉点”)的新型内存。这种内存几乎与今天的 DRAM 一样快，但它很耐用，这意味着存储在其中的数据可以像写入硬盘的数据一样在服务器重启后继续存在。

3D XPoint 内存也应该相当便宜，所以你可以以合理的价格获得大内存块——硬盘大小。3D XPoint 内存有望彻底改变需要读写大量数据的软件的速度。

### 网络接口卡(NIC)

网络接口卡(NIC)用于通过网络发送和接收数据。您计算机中网卡的速度会影响您与其他计算机通信的速度。

速度可能会影响缓存需求等。本地数据，无论是在内存中还是在磁盘上。网卡越快(连接的另一端也越快)，对缓存的需求就越少。当然，如果数据需要在途中通过其他几台计算机，就像通过互联网发送数据一样，那么所有这些中间计算机+网卡的速度也很重要。

通常，当您从网卡读取数据时，数据首先从网络读入操作系统内存，然后从操作系统内存读入应用程序的内存。但是，某些类型的网卡技术可以绕过操作系统内存复制步骤，直接将数据从网卡复制到应用程序的内存中。其中一些技术是“Infiniband”和“融合以太网 RDMA”。

## 顺序数据访问更快

由于所有的缓存层，以及普通硬盘的工作方式，顺序数据访问比任意数据访问更快。原因是，所有的缓存策略都是基于这样的假设，即你的程序将按顺序访问数据。让我多解释一下。

当 CPU 从主存中请求一个数据时，硬件认为 CPU 可能很快会请求下一个字节(或 64 位块),依此类推。由于许多内存和磁盘技术一次读取较大内存块的速度很快，所以硬件一次只复制这样的较大内存块是有意义的。如果 CPU 实际上按顺序处理内存，访问数据块中的下一个地址会更快，因为它已经存在于缓存中。

下图说明了 L1、L2 和 L3 的缓存原理:

![Cache behaviour - reading bigger consecutively stored blocks of data into the cache.](../Images/a2f7f95c67e16be698080b2fb9822be2.png)

这种大数据块加载行为适用于所有缓存层，一直到硬盘。即使是硬盘也可以从磁盘中读取更大的数据块，速度比一次读取一个字节的数据更快。

这种缓存行为意味着顺序访问数据比在任意地址访问数据要快得多。这意味着，像 Java `ArrayList`这样的数据结构比`LinkedList`更快。

当你访问内存(或磁盘)中任意地址的数据时，速度会慢很多，因为请求的数据很少位于缓存中。这意味着必须从磁盘、主内存、另一个缓存层等加载数据。这比直接在最近的高速缓存中访问(最好是 L1)要慢。